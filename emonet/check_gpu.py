"""
GPUåŠ é€Ÿæ£€æŸ¥å·¥å…·
ç”¨äºéªŒè¯GPUæ˜¯å¦æ­£ç¡®é…ç½®å’Œä½¿ç”¨
"""
import torch
import sys

def check_gpu_setup():
    """æ£€æŸ¥GPUé…ç½®å’Œä½¿ç”¨æƒ…å†µ"""
    
    print("=" * 60)
    print("GPUåŠ é€Ÿæ£€æŸ¥å·¥å…·")
    print("=" * 60)
    
    # 1. æ£€æŸ¥CUDAå¯ç”¨æ€§
    print("\n[1] CUDAå¯ç”¨æ€§æ£€æŸ¥")
    print("-" * 60)
    cuda_available = torch.cuda.is_available()
    print(f"  CUDAæ˜¯å¦å¯ç”¨: {cuda_available}")
    
    if not cuda_available:
        print("\n  âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDA GPU")
        print("  â†’ å°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        print("  â†’ å¦‚éœ€ä½¿ç”¨GPUï¼Œè¯·å®‰è£…CUDAå’ŒPyTorch GPUç‰ˆæœ¬")
        return False
    
    # 2. GPUä¿¡æ¯
    print("\n[2] GPUè®¾å¤‡ä¿¡æ¯")
    print("-" * 60)
    gpu_count = torch.cuda.device_count()
    print(f"  GPUæ•°é‡: {gpu_count}")
    
    for i in range(gpu_count):
        print(f"\n  GPU {i}:")
        print(f"    åç§°: {torch.cuda.get_device_name(i)}")
        print(f"    è®¡ç®—èƒ½åŠ›: {torch.cuda.get_device_capability(i)}")
        
        # å†…å­˜ä¿¡æ¯
        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
        allocated = torch.cuda.memory_allocated(i) / (1024**3)
        reserved = torch.cuda.memory_reserved(i) / (1024**3)
        
        print(f"    æ€»å†…å­˜: {total_memory:.2f} GB")
        print(f"    å·²åˆ†é…: {allocated:.2f} GB")
        print(f"    å·²ä¿ç•™: {reserved:.2f} GB")
        print(f"    å¯ç”¨: {total_memory - reserved:.2f} GB")
    
    # 3. å½“å‰è®¾å¤‡
    print("\n[3] å½“å‰ä½¿ç”¨è®¾å¤‡")
    print("-" * 60)
    current_device = torch.cuda.current_device()
    print(f"  å½“å‰GPU: {current_device}")
    print(f"  è®¾å¤‡åç§°: {torch.cuda.get_device_name(current_device)}")
    
    # 4. PyTorchç‰ˆæœ¬ä¿¡æ¯
    print("\n[4] PyTorchç‰ˆæœ¬ä¿¡æ¯")
    print("-" * 60)
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda if torch.version.cuda else 'N/A'}")
    print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'N/A'}")
    
    # 5. cuDNNè®¾ç½®
    print("\n[5] cuDNNä¼˜åŒ–è®¾ç½®")
    print("-" * 60)
    print(f"  cuDNNå¯ç”¨: {torch.backends.cudnn.is_available()}")
    print(f"  cuDNNå¯ç”¨: {torch.backends.cudnn.enabled}")
    print(f"  Benchmarkæ¨¡å¼: {torch.backends.cudnn.benchmark}")
    
    if not torch.backends.cudnn.benchmark:
        print("\n  ğŸ’¡ å»ºè®®: å¯ç”¨benchmarkæ¨¡å¼å¯ä»¥æé«˜æ€§èƒ½")
        print("    æ·»åŠ : torch.backends.cudnn.benchmark = True")
    
    # 6. ç®€å•æ€§èƒ½æµ‹è¯•
    print("\n[6] ç®€å•æ€§èƒ½æµ‹è¯•")
    print("-" * 60)
    
    try:
        from emonet.models import EmoNet
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_image = torch.randn(1, 3, 256, 256)
        
        # CPUæµ‹è¯•
        device_cpu = 'cpu'
        net_cpu = EmoNet(n_expression=8).to(device_cpu)
        net_cpu.eval()
        
        import time
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = net_cpu(test_image)
        
        # CPUæµ‹è¯•
        start = time.time()
        with torch.no_grad():
            for _ in range(10):
                _ = net_cpu(test_image)
        cpu_time = (time.time() - start) / 10
        
        # GPUæµ‹è¯•
        device_gpu = 'cuda:0'
        net_gpu = EmoNet(n_expression=8).to(device_gpu)
        net_gpu.eval()
        
        # é¢„çƒ­
        with torch.no_grad():
            _ = net_gpu(test_image.to(device_gpu))
        
        # GPUæµ‹è¯•
        torch.cuda.synchronize()  # ç­‰å¾…GPUå®Œæˆ
        start = time.time()
        with torch.no_grad():
            for _ in range(10):
                _ = net_gpu(test_image.to(device_gpu))
        torch.cuda.synchronize()  # ç­‰å¾…GPUå®Œæˆ
        gpu_time = (time.time() - start) / 10
        
        print(f"  CPUå•æ¬¡æ¨ç†æ—¶é—´: {cpu_time*1000:.2f} ms")
        print(f"  GPUå•æ¬¡æ¨ç†æ—¶é—´: {gpu_time*1000:.2f} ms")
        print(f"  åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")
        
        if gpu_time < cpu_time:
            print(f"  âœ… GPUåŠ é€Ÿæ­£å¸¸å·¥ä½œï¼")
        else:
            print(f"  âš ï¸  GPUä¼¼ä¹æ²¡æœ‰åŠ é€Ÿï¼ˆå¯èƒ½æ•°æ®å¤ªå°æˆ–CPUå¾ˆå¿«ï¼‰")
            
    except Exception as e:
        print(f"  æ— æ³•è¿è¡Œæ€§èƒ½æµ‹è¯•: {e}")
    
    print("\n" + "=" * 60)
    print("æ£€æŸ¥å®Œæˆï¼")
    print("=" * 60)
    
    return True

if __name__ == "__main__":
    success = check_gpu_setup()
    sys.exit(0 if success else 1)


